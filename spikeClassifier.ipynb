{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48da973b",
   "metadata": {},
   "source": [
    "# Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02ed21b-a28d-4bf9-98c1-b3539408f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import time\n",
    "\n",
    "# Import custom utility functions\n",
    "import glycan_bionames\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "# Define custom functions\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "def restrict_RBD_window(df,nm):\n",
    "    '''Function to drop features of dataframe that correspond to glycans which are outside a given RBD neighborhood (in nm)'''\n",
    "    #Get list of glycans\n",
    "    glycans = list(np.unique([x.replace('RBD__2__','') for x in df.keys().to_list() if 'RBD__2__GLY' in x]))\n",
    "    \n",
    "    for g in glycans:\n",
    "        if df['RBD__2__' + g].mean() > nm:\n",
    "            df.drop(['RBD__2__'+g,g+':ROF',g+':RMSD',g+'_x',g+'_y',g+'_z'],axis=1,inplace=True)    \n",
    "    return df\n",
    "\n",
    "def overlapping_hist(open_df,closed_df,feat):\n",
    "    '''Plot overlapping histograms for a given feature of all datasets'''\n",
    "    open_df[feat].hist(bins=50)\n",
    "    closed_df[feat].hist(bins=50)\n",
    "    mutant_df[feat].hist(bins=50)\n",
    "    plt.legend(['Open','Closed','Mutant (open)'])\n",
    "    plt.title(feat)\n",
    "    if 'RBD__2__' in feat:\n",
    "        plt.xlabel('nm')\n",
    "        \n",
    "def drop_feats(df,flag):\n",
    "    '''Drops all features in df containing flag'''\n",
    "    for f in df.keys().to_list():\n",
    "        if flag in f:\n",
    "            df.drop(f,axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "def read_n_filter_dfs(fname,num_reps,RBD_wind,val_reps_open,val_reps_closed,label_val,dfs_train=[],dfs_val=[]):\n",
    "    '''Reads data and filters columns, then places in either train or validation dataframe list'''\n",
    "    for i in range(1,num_reps+1):\n",
    "        df = pd.read_csv(fname+'_'+str(i)+'.csv').assign(label=label_val).iloc[:,1:]\n",
    "        # Only use glycans within certain range of the RBD\n",
    "        df = restrict_RBD_window(df,RBD_wind)\n",
    "        # Drop _x, _y, and _z features\n",
    "        df = drop_feats(df,'_x')\n",
    "        df = drop_feats(df,'_y')\n",
    "        df = drop_feats(df,'_z')\n",
    "        # Withold some replicants for use in a separate validation set\n",
    "        if (label_val==1) & (i in val_reps_open):\n",
    "            dfs_val.append(df)\n",
    "        elif (label_val==0) & (i in val_reps_closed):\n",
    "            dfs_val.append(df)\n",
    "        else:\n",
    "            dfs_train.append(df)\n",
    "            \n",
    "    return dfs_train, dfs_val\n",
    "\n",
    "def remove_corr_feats(full_df,corr_thresh= 0.65):\n",
    "    '''Remove highly correlated features'''\n",
    "    corr_matrix = full_df.corr()\n",
    "    final_features = corr_matrix['RBD_CA0:RMSD'][(corr_matrix['RBD_CA0:RMSD'] < corr_thresh) & (corr_matrix['RBD_CA0:RMSD'] > -corr_thresh)].reset_index().loc[:,'index'].to_list()\n",
    "    if 'label' not in final_features:\n",
    "        final_features.append('label')\n",
    "    clf_df = full_df.loc[:,final_features]\n",
    "    return clf_df\n",
    "\n",
    "def prep_ML_data(clf_df,ts,rs,labelnames):\n",
    "    '''Prepare data for use in training machine learning algorithm'''\n",
    "    # Split training and testing data\n",
    "    train_set, test_set = train_test_split(clf_df,test_size=ts, random_state=rs,stratify=labelnames)\n",
    "    print(f'Train set : {train_set.shape}, Test set : {test_set.shape}')\n",
    "\n",
    "    # Split data and labels\n",
    "    train_X = train_set.drop(\"label\", axis=1) # drop labels for training set\n",
    "    train_labels = train_set[\"label\"].copy()\n",
    "    test_X = test_set.drop(\"label\", axis=1) # drop labels for training set\n",
    "    test_labels = test_set[\"label\"].copy()\n",
    "\n",
    "    return train_X, test_X, train_labels, test_labels\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3fecd",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ccdfad",
   "metadata": {},
   "source": [
    "### All as one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca32ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dataset\n",
    "fname = '/net/jam-amaro-shared/dse_project/Spike_Dataset/TRAJECTORIES_spike_open_prot_glyc_amarolab/results/FinalExtractedFeature_open.csv'\n",
    "open_df = pd.read_csv(fname).assign(label = 1).iloc[:,1:]\n",
    "\n",
    "# Closed dataset\n",
    "fname = '/net/jam-amaro-shared/dse_project/Spike_Dataset/TRAJECTORIES_spike_closed_prot_glyc_amarolab/results/FinalExtractedFeature_closed.csv'\n",
    "closed_df = pd.read_csv(fname).assign(label = 0).iloc[:,1:]\n",
    "\n",
    "# Mutant dataset\n",
    "fname = '/net/jam-amaro-shared/dse_project/Spike_Dataset/TRAJECTORIES_spike_mutant_prot_glyc_amarolab.tar.gz/results/FinalExtractedFeature_mutant.csv'\n",
    "#mutant_df = pd.read_csv(fname).assign(label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82aecd",
   "metadata": {},
   "source": [
    "# Filter out features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use glycans within 10 nm of the RBD\n",
    "open_df = restrict_RBD_window(open_df,8)\n",
    "closed_df = restrict_RBD_window(closed_df,8)\n",
    "#mutant_df = restrict_RBD_window(mutant_df,8)\n",
    "print(open_df.shape)\n",
    "\n",
    "# Drop _x, _y, _z features\n",
    "open_df = drop_feats(open_df,'_x')\n",
    "open_df = drop_feats(open_df,'_y')\n",
    "open_df = drop_feats(open_df,'_z')\n",
    "\n",
    "closed_df = drop_feats(closed_df,'_x')\n",
    "closed_df = drop_feats(closed_df,'_y')\n",
    "closed_df = drop_feats(closed_df,'_z')\n",
    "\n",
    "# Only use columns that exist in all datasets\n",
    "common_cols = set(open_df.columns.to_list()).intersection(closed_df.columns.to_list())\n",
    "full_df = open_df.loc[:,common_cols].append(closed_df.loc[:,common_cols]).drop(['frame','Frame Num'],axis=1)\n",
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e9e68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23526, 63)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_df = remove_corr_feats(full_df,0.5)\n",
    "clf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda6938-3b4d-48f1-8386-847fce32375f",
   "metadata": {},
   "source": [
    "# Prepare the Data for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b13671cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set : (16468, 63), Test set : (7058, 63)\n"
     ]
    }
   ],
   "source": [
    "# Split train/test data\n",
    "train_X, test_X, train_labels, test_labels = prep_ML_data(clf_df,0.3,42,full_df.label)\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "num_pipeline = Pipeline([\n",
    "       ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "train_X_prepared = num_pipeline.fit_transform(train_X)\n",
    "test_X_prepared = num_pipeline.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d437c",
   "metadata": {},
   "source": [
    "# Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa20e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "8.309174060821533 sec elapsed\n",
      " Train precison : 1.0, train recall 1.0\n",
      " Test precison : 1.0, Test recall 1.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize classifier\n",
    "sgd_clf = SGDClassifier(max_iter=100, tol=1e-3, random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation on training data\n",
    "y_train_pred = cross_val_predict(sgd_clf,train_X_prepared, train_labels, cv=10)\n",
    "t = time.time()\n",
    "print(cross_val_score(sgd_clf, train_X_prepared, train_labels, cv=10, scoring=\"accuracy\"))\n",
    "print(str(time.time()-t) + ' sec elapsed')\n",
    "\n",
    "# Get overall precision and recall for training data\n",
    "confusion_matrix(train_labels, y_train_pred)\n",
    "print(f' Train precison : {precision_score(train_labels, y_train_pred)}, train recall {recall_score(train_labels, y_train_pred)}')\n",
    "\n",
    "# Get overall precision and recall for testing data\n",
    "sgd_clf.fit(train_X_prepared,train_labels)\n",
    "y_test_pred = sgd_clf.predict(test_X_prepared)\n",
    "print(f' Test precison : {precision_score(test_labels, y_test_pred)}, Test recall {recall_score(test_labels, y_test_pred)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2492fae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#mutant_df.loc[:,common_cols].drop([\"frame\",\"Frame Num\"],axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0dd266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#val_X = num_pipeline.transform(mutant_df.loc[:,train_set.keys().to_list()].drop([\"label\"],axis=1))\n",
    "#val_labels = mutant_df[\"label\"].copy()\n",
    "#y_val_pred = sgd_clf.predict(val_X)\n",
    "#print(f' Val precison : {precision_score(val_labels, y_val_pred)}, Val recall {recall_score(val_labels, y_val_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edcabfd",
   "metadata": {},
   "source": [
    "# Iterative Replicant Analysis\n",
    "\n",
    "Run iterative leave-one-out analysis wherein 1/3 of the replicants are withheld from the training/testing dataset and used as a separate \"validation\" dataset afterwards. The idea is to implement the trained model on a completely \"new\" dataset and see if the model's performance holds up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7f3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set : (934017, 79), Test set : (400293, 79)\n",
      "[1.         1.         1.         1.         1.         0.99998929\n",
      " 0.99997859 1.         0.99998929 1.        ]\n",
      "22.714370489120483 sec elapsed\n",
      " Train precison : 0.9999939197713834, train recall 1.0\n",
      " Test precison : 0.999964532970151, Test recall 1.0\n",
      " Val precison : 0.9999738670494853, Val recall 1.0\n",
      "Train set : (957670, 79), Test set : (410430, 79)\n"
     ]
    }
   ],
   "source": [
    "RBD_wind = 8\n",
    "leftouts = []\n",
    "train_precs = np.zeros([6,3])\n",
    "train_recalls = np.zeros([6,3])\n",
    "test_precs = np.zeros([6,3])\n",
    "test_recalls = np.zeros([6,3])\n",
    "val_precs = np.zeros([6,3])\n",
    "val_recalls = np.zeros([6,3])\n",
    "top_feats = []\n",
    "for i in range(1,7):\n",
    "    for j in range(1,4):\n",
    "        val_reps_closed = [j]\n",
    "        if i == 6:\n",
    "            val_reps_open = [1,6];\n",
    "        else:\n",
    "            val_reps_open = [i,i+1];\n",
    "\n",
    "        # Read open data\n",
    "        fname = '/net/jam-amaro-shared/dse_project/Spike_Dataset/TRAJECTORIES_spike_open_prot_glyc_amarolab/results/FinalExtractedFeature'\n",
    "        dfs_train, dfs_val = read_n_filter_dfs(fname,6,RBD_wind,val_reps_open,val_reps_closed,1)\n",
    "\n",
    "        # Read closed data\n",
    "        fname = fname = '/net/jam-amaro-shared/dse_project/Spike_Dataset/TRAJECTORIES_spike_closed_prot_glyc_amarolab/results/FinalExtractedFeature'\n",
    "        dfs_train, dfs_val = read_n_filter_dfs(fname,3,RBD_wind,val_reps_open,val_reps_closed,0,dfs_train,dfs_val)\n",
    "        \n",
    "        # Only use columns that exist in all datasets\n",
    "        common_cols = list(set.intersection(*map(set,dfs_train+dfs_val)))\n",
    "        full_df = pd.concat(dfs_train).loc[:,common_cols].drop(['frame'],axis = 1)\n",
    "        full_df.shape\n",
    "        \n",
    "        # Remove highly correlated columns\n",
    "        clf_df = remove_corr_feats(full_df,0.5)\n",
    "\n",
    "        # Split train/test data\n",
    "        train_X, test_X, train_labels, test_labels = prep_ML_data(clf_df,0.3,42,full_df.label)\n",
    "\n",
    "\n",
    "        # Normalize data\n",
    "        num_pipeline = Pipeline([\n",
    "               ('std_scaler', StandardScaler()),\n",
    "            ])\n",
    "        train_X_prepared = num_pipeline.fit_transform(train_X)\n",
    "        test_X_prepared = num_pipeline.transform(test_X)\n",
    "        \n",
    "        # Initialize classifier\n",
    "        sgd_clf = SGDClassifier(max_iter=100, tol=1e-3, random_state=42)\n",
    "\n",
    "        # Perform 10-fold cross-validation on training data\n",
    "        y_train_pred = cross_val_predict(sgd_clf,train_X_prepared, train_labels, cv=10)\n",
    "        t = time.time()\n",
    "        print(cross_val_score(sgd_clf, train_X_prepared, train_labels, cv=10, scoring=\"accuracy\"))\n",
    "        print(str(time.time()-t) + ' sec elapsed')\n",
    "\n",
    "        # Get overall precision and recall for training data\n",
    "        confusion_matrix(train_labels, y_train_pred)\n",
    "        print(f' Train precison : {precision_score(train_labels, y_train_pred)}, train recall {recall_score(train_labels, y_train_pred)}')\n",
    "\n",
    "        # Get overall precision and recall for testing data\n",
    "        sgd_clf.fit(train_X_prepared,train_labels)\n",
    "        y_test_pred = sgd_clf.predict(test_X_prepared)\n",
    "        print(f' Test precison : {precision_score(test_labels, y_test_pred)}, Test recall {recall_score(test_labels, y_test_pred)}')\n",
    "\n",
    "        # Prep data\n",
    "        val_X = pd.concat(dfs_val).loc[:,train_X.keys()]\n",
    "        val_labels = pd.concat(dfs_val).label\n",
    "        val_X_prepared = num_pipeline.transform(val_X)\n",
    "\n",
    "        # Get testing results on unseen replicant(s)\n",
    "        y_val_pred = sgd_clf.predict(val_X_prepared)\n",
    "        print(f' Val precison : {precision_score(val_labels, y_val_pred)}, Val recall {recall_score(val_labels, y_val_pred)}')\n",
    "        \n",
    "        # Save results\n",
    "        leftouts.append(['open '+ str(x) +' ' for x in val_reps_open] + ['closed ' + str(x) + ' ' for x in val_reps_closed])\n",
    "        train_precs[i-1,j-1] = precision_score(train_labels, y_train_pred)\n",
    "        train_recalls[i-1,j-1] = recall_score(train_labels,y_train_pred)\n",
    "        test_precs[i-1,j-1] = precision_score(test_labels, y_test_pred)\n",
    "        test_recalls[i-1,j-1] = recall_score(test_labels, y_test_pred)\n",
    "        val_precs[i-1,j-1] = precision_score(val_labels, y_val_pred)\n",
    "        val_recalls[i-1,j-1] = recall_score(val_labels, y_val_pred)\n",
    "        a = list(np.abs(sgd_clf.coef_[0]))\n",
    "        idx = sorted(range(len(a)), key = lambda k: a[k])[-5:]\n",
    "        x_vals = [glycan_bionames.get_elem(k,'feat') for k in train_X.columns.to_list()]\n",
    "        top_feats.append(list(np.array(x_vals)[idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750eaaf-8215-4f7c-a3fc-21fdee87b7ef",
   "metadata": {},
   "source": [
    "# Present Results Graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a37a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(np.abs(sgd_clf.coef_[0]))\n",
    "idx = sorted(range(len(a)), key = lambda k: a[k])[-5:]\n",
    "x_vals = [glycan_bionames.get_elem(i,'feat') for i in train_X.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e888f-b861-43a8-bdf4-2ec963f8263d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_vals = [glycan_bionames.get_elem(i,'feat') for i in train_X.columns.to_list()]\n",
    "#x_vals = train_X.columns.to_list()\n",
    "y_vals = np.abs(sgd_clf.coef_[0])\n",
    "col_vals = [glycan_bionames.get_elem(i,'chain') for i in train_X.columns.to_list()]\n",
    "\n",
    "px.bar(x=x_vals,y=y_vals,color=col_vals,labels={'x':'Feature','y':'Importance','color':'Importance'}).update_xaxes(categoryorder='total ascending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overlapping histograms for some features\n",
    "\n",
    "feats = list(np.unique([x for x in train_set.keys().to_list() if 'RBD__2__GLY' in x]))        \n",
    "#feats = ['RBD__2__GLY51','RBD__2__GLY24','RBD__2__GLY14','RBD__2__GLY3']   \n",
    "#feats = ['GLY22:ROF','RBD__2__CH_CA0','RBD__2__backbone0','RBD__2__GLY32','RBD__2__GLY51']\n",
    "plt.figure(figsize=(30,18))\n",
    "for i in range(len(feats)):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    overlapping_hist(open_df,closed_df,feats[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba966c8-5714-40b2-aaaa-f59c22f260dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of all features in full training dataset\n",
    "train_X.hist(bins=50, figsize=(20,15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
